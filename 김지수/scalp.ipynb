{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'Training'\n",
    "train_json = glob.glob(train_dir + '/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72342"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_json)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Val 1 미세각질\n",
    "Val 2 피지과다\n",
    "Val 3 모낭사이홍반\n",
    "Val 4 모낭홍반/농포\n",
    "Val 5 비듬\n",
    "Val 6 탈모\n",
    "\n",
    "각각 4가지 상태가 있는데, 이를 모두 고려한다면 4096 가지의 classification을 진행해야함. 따라서 각 상태에 대해서 증상 유, 무만 고려할 예정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': '0013_A2LEBJJDE00060O_1602578303771_2',\n",
       " 'image_file_name': '0013_A2LEBJJDE00060O_1602578303771_2_TH.jpg',\n",
       " 'value_1': '0',\n",
       " 'value_2': '1',\n",
       " 'value_3': '2',\n",
       " 'value_4': '0',\n",
       " 'value_5': '1',\n",
       " 'value_6': '0'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status = ['value_1', 'value_2', 'value_3', 'value_4', 'value_5', 'value_6']\n",
    "with open(train_json[0]) as j:\n",
    "    example = json.load(j)\n",
    "    \n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_1, y_train_1 = [], []\n",
    "# x_train_2, y_train_2 = [], []\n",
    "# x_train_3, y_train_3 = [], []\n",
    "# x_train_4, y_train_4 = [], []\n",
    "# x_train_5, y_train_5 = [], []\n",
    "# x_train_6, y_train_6 = [], []\n",
    "# for name in train_json[10:]:\n",
    "#     with open(name, 'r',encoding='utf-8') as j:\n",
    "#         data = json.load(j)\n",
    "#         f_name = data['image_file_name']\n",
    "#         # print(train_dir+ '/' + f_name)\n",
    "#         img = cv2.imread(train_dir+ '/' + f_name, cv2.IMREAD_COLOR)\n",
    "#         # print(img)\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         img = img[80:560,:]\n",
    "#         img = cv2.resize(img, (64,64))\n",
    "#         cv2.imwrite('Training/'+f_name,img)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = 'test.h5'\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25,restore_best_weights=True)\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(saved_model, monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 65540     \n",
      "=================================================================\n",
      "Total params: 436,356\n",
      "Trainable params: 436,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = tf.keras.models.Sequential()\n",
    "#\n",
    "model1.add(tf.keras.layers.Conv2D(64,3,padding='same',activation='relu',input_shape=(64,64,3)))  # conv2d 수행하면 64채널의 결과가 나오고, kernelsize = 3\n",
    "model1.add(tf.keras.layers.Dropout(rate=0.5))                          #  DO1\n",
    "model1.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "#\n",
    "model1.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "model1.add(tf.keras.layers.Dropout(rate=0.5))                          # DO2\n",
    "model1.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "#\n",
    "model1.add(tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'))\n",
    "model1.add(tf.keras.layers.Dropout(rate=0.5))                           # DO3\n",
    "model1.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "#\n",
    "model1.add(tf.keras.layers.Flatten())\n",
    "model1.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "#\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "               \n",
    "               loss='sparse_categorical_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "201/201 [==============================] - 9s 13ms/step - loss: 0.6337 - accuracy: 0.8510 - val_loss: 1.0721 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.85945, saving model to test.h5\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5871 - accuracy: 0.8523 - val_loss: 1.0432 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.85945\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5893 - accuracy: 0.8523 - val_loss: 0.9876 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.85945\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5924 - accuracy: 0.8523 - val_loss: 0.9514 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.85945\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5797 - accuracy: 0.8523 - val_loss: 0.8683 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.85945\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5776 - accuracy: 0.8523 - val_loss: 0.9395 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.85945\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5772 - accuracy: 0.8523 - val_loss: 0.9010 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.85945\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5731 - accuracy: 0.8523 - val_loss: 0.8684 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.85945\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5803 - accuracy: 0.8523 - val_loss: 0.8733 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.85945\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5749 - accuracy: 0.8523 - val_loss: 0.7768 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.85945\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5644 - accuracy: 0.8523 - val_loss: 0.7237 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.85945\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5539 - accuracy: 0.8519 - val_loss: 0.6902 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.85945\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5500 - accuracy: 0.8523 - val_loss: 0.7813 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.85945\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5385 - accuracy: 0.8523 - val_loss: 0.6739 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.85945\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5222 - accuracy: 0.8535 - val_loss: 0.7071 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.85945\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 2s 12ms/step - loss: 0.5037 - accuracy: 0.8523 - val_loss: 0.6958 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.85945\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.4884 - accuracy: 0.8554 - val_loss: 0.6648 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.85945\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 2s 12ms/step - loss: 0.4622 - accuracy: 0.8588 - val_loss: 0.6738 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.85945\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 2s 12ms/step - loss: 0.4473 - accuracy: 0.8619 - val_loss: 0.6892 - val_accuracy: 0.8557\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.85945\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 3s 13ms/step - loss: 0.4201 - accuracy: 0.8669 - val_loss: 0.7087 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.85945\n",
      "Epoch 21/50\n",
      "201/201 [==============================] - 2s 12ms/step - loss: 0.3916 - accuracy: 0.8728 - val_loss: 0.6745 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.85945\n",
      "Epoch 22/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3775 - accuracy: 0.8743 - val_loss: 0.6557 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.85945\n",
      "Epoch 23/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3477 - accuracy: 0.8852 - val_loss: 0.6732 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.85945\n",
      "Epoch 24/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3286 - accuracy: 0.8855 - val_loss: 0.6456 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.85945\n",
      "Epoch 25/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2941 - accuracy: 0.8967 - val_loss: 0.6467 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.85945\n",
      "Epoch 26/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2779 - accuracy: 0.9020 - val_loss: 0.6548 - val_accuracy: 0.8433\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.85945\n",
      "Epoch 27/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2595 - accuracy: 0.9039 - val_loss: 0.6647 - val_accuracy: 0.8346\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.85945\n",
      "Epoch 28/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2371 - accuracy: 0.9185 - val_loss: 0.6324 - val_accuracy: 0.8433\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.85945\n",
      "Epoch 29/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2213 - accuracy: 0.9210 - val_loss: 0.6755 - val_accuracy: 0.8234\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.85945\n",
      "Epoch 30/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2201 - accuracy: 0.9182 - val_loss: 0.6888 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.85945\n",
      "Epoch 31/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2069 - accuracy: 0.9244 - val_loss: 0.6530 - val_accuracy: 0.8433\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.85945\n",
      "Epoch 32/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.1808 - accuracy: 0.9350 - val_loss: 0.6599 - val_accuracy: 0.8308\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.85945\n",
      "Epoch 33/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1709 - accuracy: 0.9359 - val_loss: 0.7052 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.85945\n",
      "Epoch 34/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1597 - accuracy: 0.9412 - val_loss: 0.7043 - val_accuracy: 0.7973\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.85945\n",
      "Epoch 35/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1383 - accuracy: 0.9496 - val_loss: 0.6617 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.85945\n",
      "Epoch 36/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1397 - accuracy: 0.9509 - val_loss: 0.6688 - val_accuracy: 0.8284\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.85945\n",
      "Epoch 37/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1395 - accuracy: 0.9471 - val_loss: 0.6669 - val_accuracy: 0.8371\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.85945\n",
      "Epoch 38/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.1356 - accuracy: 0.9484 - val_loss: 0.6763 - val_accuracy: 0.8284\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.85945\n",
      "Epoch 39/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1126 - accuracy: 0.9577 - val_loss: 0.6730 - val_accuracy: 0.8234\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.85945\n",
      "Epoch 40/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.0961 - accuracy: 0.9649 - val_loss: 0.6865 - val_accuracy: 0.8296\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.85945\n",
      "Epoch 41/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.0920 - accuracy: 0.9652 - val_loss: 0.7121 - val_accuracy: 0.8109\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.85945\n",
      "Epoch 42/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1086 - accuracy: 0.9596 - val_loss: 0.6939 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.85945\n",
      "Epoch 43/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.0958 - accuracy: 0.9633 - val_loss: 0.7046 - val_accuracy: 0.8371\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.85945\n",
      "Epoch 44/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.0823 - accuracy: 0.9701 - val_loss: 0.6994 - val_accuracy: 0.8259\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.85945\n",
      "Epoch 45/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.0825 - accuracy: 0.9708 - val_loss: 0.7404 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.85945\n",
      "Epoch 46/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.0931 - accuracy: 0.9652 - val_loss: 0.7336 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.85945\n",
      "Epoch 47/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.0715 - accuracy: 0.9760 - val_loss: 0.7305 - val_accuracy: 0.8060\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.85945\n",
      "Epoch 48/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.0656 - accuracy: 0.9767 - val_loss: 0.7255 - val_accuracy: 0.8271\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.85945\n",
      "Epoch 49/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.0719 - accuracy: 0.9736 - val_loss: 0.7418 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.85945\n",
      "Epoch 50/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.0768 - accuracy: 0.9723 - val_loss: 0.7343 - val_accuracy: 0.8296\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.85945\n",
      "Epoch 1/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.8433 - accuracy: 0.7885 - val_loss: 1.0570 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.85945\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.7320 - accuracy: 0.7972 - val_loss: 0.9882 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.85945\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.7207 - accuracy: 0.7972 - val_loss: 1.0053 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.85945\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.7108 - accuracy: 0.7972 - val_loss: 1.0054 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.85945\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.7143 - accuracy: 0.7969 - val_loss: 0.9356 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.85945\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.7047 - accuracy: 0.7972 - val_loss: 0.9587 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.85945\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.7089 - accuracy: 0.7969 - val_loss: 0.9343 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.85945\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6905 - accuracy: 0.7975 - val_loss: 0.8951 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.85945\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6769 - accuracy: 0.7956 - val_loss: 0.9093 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.85945\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6667 - accuracy: 0.7972 - val_loss: 0.9368 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.85945\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6520 - accuracy: 0.7991 - val_loss: 0.9002 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.85945\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6344 - accuracy: 0.7988 - val_loss: 0.9225 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.85945\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6183 - accuracy: 0.8006 - val_loss: 0.8789 - val_accuracy: 0.7724\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.85945\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5980 - accuracy: 0.8012 - val_loss: 0.8841 - val_accuracy: 0.7687\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.85945\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5838 - accuracy: 0.8062 - val_loss: 0.8363 - val_accuracy: 0.7711\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.85945\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5547 - accuracy: 0.8106 - val_loss: 0.8405 - val_accuracy: 0.7687\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.85945\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5316 - accuracy: 0.8140 - val_loss: 0.8293 - val_accuracy: 0.7687\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.85945\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4979 - accuracy: 0.8233 - val_loss: 0.8225 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.85945\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.4688 - accuracy: 0.8330 - val_loss: 0.8630 - val_accuracy: 0.7612\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.85945\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4518 - accuracy: 0.8355 - val_loss: 0.8510 - val_accuracy: 0.7575\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.85945\n",
      "Epoch 21/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4288 - accuracy: 0.8457 - val_loss: 0.8279 - val_accuracy: 0.7687\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.85945\n",
      "Epoch 22/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4020 - accuracy: 0.8526 - val_loss: 0.8550 - val_accuracy: 0.7562\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.85945\n",
      "Epoch 23/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3800 - accuracy: 0.8616 - val_loss: 0.8498 - val_accuracy: 0.7525\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.85945\n",
      "Epoch 24/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3462 - accuracy: 0.8799 - val_loss: 0.8390 - val_accuracy: 0.7537\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.85945\n",
      "Epoch 25/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3440 - accuracy: 0.8681 - val_loss: 0.8715 - val_accuracy: 0.7463\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.85945\n",
      "Epoch 26/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3171 - accuracy: 0.8759 - val_loss: 0.8613 - val_accuracy: 0.7475\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.85945\n",
      "Epoch 27/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2926 - accuracy: 0.8908 - val_loss: 0.8681 - val_accuracy: 0.7525\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.85945\n",
      "Epoch 28/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2765 - accuracy: 0.8955 - val_loss: 0.8713 - val_accuracy: 0.7363\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.85945\n",
      "Epoch 29/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2468 - accuracy: 0.9064 - val_loss: 0.8334 - val_accuracy: 0.7537\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.85945\n",
      "Epoch 30/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2453 - accuracy: 0.9082 - val_loss: 0.8890 - val_accuracy: 0.7438\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.85945\n",
      "Epoch 31/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2662 - accuracy: 0.9030 - val_loss: 0.8732 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.85945\n",
      "Epoch 32/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2069 - accuracy: 0.9269 - val_loss: 0.8628 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.85945\n",
      "Epoch 33/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2031 - accuracy: 0.9213 - val_loss: 0.8661 - val_accuracy: 0.7537\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.85945\n",
      "Epoch 34/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1925 - accuracy: 0.9281 - val_loss: 0.8770 - val_accuracy: 0.7450\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.85945\n",
      "Epoch 35/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1874 - accuracy: 0.9344 - val_loss: 0.8737 - val_accuracy: 0.7376\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.85945\n",
      "Epoch 36/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2002 - accuracy: 0.9294 - val_loss: 0.8978 - val_accuracy: 0.7438\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.85945\n",
      "Epoch 37/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1565 - accuracy: 0.9421 - val_loss: 0.9252 - val_accuracy: 0.7363\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.85945\n",
      "Epoch 38/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1607 - accuracy: 0.9421 - val_loss: 0.9023 - val_accuracy: 0.7413\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.85945\n",
      "Epoch 39/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1578 - accuracy: 0.9415 - val_loss: 0.9596 - val_accuracy: 0.7226\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.85945\n",
      "Epoch 40/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1501 - accuracy: 0.9493 - val_loss: 0.9602 - val_accuracy: 0.7413\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.85945\n",
      "Epoch 41/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1369 - accuracy: 0.9537 - val_loss: 0.9546 - val_accuracy: 0.7338\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.85945\n",
      "Epoch 42/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.1305 - accuracy: 0.9521 - val_loss: 1.0652 - val_accuracy: 0.7313\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.85945\n",
      "Epoch 43/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1334 - accuracy: 0.9537 - val_loss: 0.9889 - val_accuracy: 0.7313\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.85945\n",
      "Epoch 00043: early stopping\n",
      "Epoch 1/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6956 - accuracy: 0.8100 - val_loss: 0.8569 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.85945\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6648 - accuracy: 0.8168 - val_loss: 0.8630 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.85945\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6459 - accuracy: 0.8168 - val_loss: 0.8471 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.85945\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6442 - accuracy: 0.8174 - val_loss: 0.7875 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.85945\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6353 - accuracy: 0.8171 - val_loss: 0.7865 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.85945\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6194 - accuracy: 0.8174 - val_loss: 0.7671 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.85945\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6142 - accuracy: 0.8180 - val_loss: 0.7925 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.85945\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6044 - accuracy: 0.8196 - val_loss: 0.7578 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.85945\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5812 - accuracy: 0.8205 - val_loss: 0.7887 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.85945\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5808 - accuracy: 0.8196 - val_loss: 0.7300 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.85945\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5584 - accuracy: 0.8230 - val_loss: 0.7550 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.85945\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5522 - accuracy: 0.8236 - val_loss: 0.7144 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.85945\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5283 - accuracy: 0.8252 - val_loss: 0.7968 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.85945\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.4943 - accuracy: 0.8327 - val_loss: 0.7732 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.85945\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4829 - accuracy: 0.8311 - val_loss: 0.6844 - val_accuracy: 0.8308\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.85945\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.4593 - accuracy: 0.8392 - val_loss: 0.7536 - val_accuracy: 0.8271\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.85945\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.4353 - accuracy: 0.8439 - val_loss: 0.6779 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.85945\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4183 - accuracy: 0.8482 - val_loss: 0.7105 - val_accuracy: 0.8197\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.85945\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3955 - accuracy: 0.8569 - val_loss: 0.6912 - val_accuracy: 0.8234\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.85945\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3537 - accuracy: 0.8737 - val_loss: 0.6981 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.85945\n",
      "Epoch 21/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3449 - accuracy: 0.8719 - val_loss: 0.7302 - val_accuracy: 0.8122\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.85945\n",
      "Epoch 22/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3276 - accuracy: 0.8778 - val_loss: 0.7093 - val_accuracy: 0.8134\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.85945\n",
      "Epoch 23/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3214 - accuracy: 0.8796 - val_loss: 0.7027 - val_accuracy: 0.8134\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.85945\n",
      "Epoch 24/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2753 - accuracy: 0.9023 - val_loss: 0.6738 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.85945\n",
      "Epoch 25/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2631 - accuracy: 0.9026 - val_loss: 0.7069 - val_accuracy: 0.8134\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.85945\n",
      "Epoch 26/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2636 - accuracy: 0.9048 - val_loss: 0.6952 - val_accuracy: 0.8259\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.85945\n",
      "Epoch 27/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2547 - accuracy: 0.8995 - val_loss: 0.7162 - val_accuracy: 0.8246\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.85945\n",
      "Epoch 28/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2304 - accuracy: 0.9176 - val_loss: 0.7213 - val_accuracy: 0.8109\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.85945\n",
      "Epoch 29/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2135 - accuracy: 0.9176 - val_loss: 0.7552 - val_accuracy: 0.8109\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.85945\n",
      "Epoch 30/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1926 - accuracy: 0.9285 - val_loss: 0.7248 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.85945\n",
      "Epoch 31/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.1959 - accuracy: 0.9341 - val_loss: 0.7280 - val_accuracy: 0.8072\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.85945\n",
      "Epoch 32/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1877 - accuracy: 0.9272 - val_loss: 0.7602 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.85945\n",
      "Epoch 33/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1724 - accuracy: 0.9319 - val_loss: 0.7666 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.85945\n",
      "Epoch 34/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1610 - accuracy: 0.9418 - val_loss: 0.7453 - val_accuracy: 0.8047\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.85945\n",
      "Epoch 35/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1594 - accuracy: 0.9409 - val_loss: 0.7285 - val_accuracy: 0.8035\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.85945\n",
      "Epoch 36/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1619 - accuracy: 0.9397 - val_loss: 0.7390 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.85945\n",
      "Epoch 37/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1613 - accuracy: 0.9409 - val_loss: 0.8352 - val_accuracy: 0.8122\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.85945\n",
      "Epoch 38/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1521 - accuracy: 0.9428 - val_loss: 0.8089 - val_accuracy: 0.8246\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.85945\n",
      "Epoch 39/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1406 - accuracy: 0.9459 - val_loss: 0.7764 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.85945\n",
      "Epoch 40/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1307 - accuracy: 0.9515 - val_loss: 0.7772 - val_accuracy: 0.8060\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.85945\n",
      "Epoch 41/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1412 - accuracy: 0.9474 - val_loss: 0.8287 - val_accuracy: 0.8234\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.85945\n",
      "Epoch 42/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1185 - accuracy: 0.9565 - val_loss: 0.8210 - val_accuracy: 0.7960\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.85945\n",
      "Epoch 43/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1284 - accuracy: 0.9558 - val_loss: 0.7759 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.85945\n",
      "Epoch 44/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1196 - accuracy: 0.9565 - val_loss: 0.8672 - val_accuracy: 0.8184\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.85945\n",
      "Epoch 45/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1353 - accuracy: 0.9518 - val_loss: 0.7492 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.85945\n",
      "Epoch 46/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1242 - accuracy: 0.9571 - val_loss: 0.8171 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.85945\n",
      "Epoch 47/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1116 - accuracy: 0.9549 - val_loss: 0.7496 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.85945\n",
      "Epoch 48/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1046 - accuracy: 0.9627 - val_loss: 0.8069 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.85945\n",
      "Epoch 49/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.0909 - accuracy: 0.9680 - val_loss: 0.8989 - val_accuracy: 0.8159\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.85945\n",
      "Epoch 00049: early stopping\n",
      "Epoch 1/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5827 - accuracy: 0.8631 - val_loss: 0.7155 - val_accuracy: 0.8731\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.85945 to 0.87313, saving model to test.h5\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5251 - accuracy: 0.8675 - val_loss: 0.7535 - val_accuracy: 0.8731\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.87313\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5173 - accuracy: 0.8663 - val_loss: 0.6498 - val_accuracy: 0.8731\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.87313\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5029 - accuracy: 0.8669 - val_loss: 0.6641 - val_accuracy: 0.8731\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.87313\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4877 - accuracy: 0.8672 - val_loss: 0.6193 - val_accuracy: 0.8731\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.87313\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4812 - accuracy: 0.8675 - val_loss: 0.6171 - val_accuracy: 0.8731\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.87313\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.4713 - accuracy: 0.8675 - val_loss: 0.6037 - val_accuracy: 0.8731\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.87313\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.4627 - accuracy: 0.8700 - val_loss: 0.6408 - val_accuracy: 0.8731\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.87313\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.4480 - accuracy: 0.8709 - val_loss: 0.6485 - val_accuracy: 0.8731\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.87313\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.4469 - accuracy: 0.8712 - val_loss: 0.6632 - val_accuracy: 0.8731\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.87313\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 3s 13ms/step - loss: 0.4288 - accuracy: 0.8715 - val_loss: 0.6098 - val_accuracy: 0.8731\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.87313\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 3s 16ms/step - loss: 0.4144 - accuracy: 0.8734 - val_loss: 0.6384 - val_accuracy: 0.8731\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.87313\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 3s 15ms/step - loss: 0.3980 - accuracy: 0.8771 - val_loss: 0.6208 - val_accuracy: 0.8719\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.87313\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3781 - accuracy: 0.8793 - val_loss: 0.5744 - val_accuracy: 0.8731\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.87313\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3608 - accuracy: 0.8846 - val_loss: 0.6339 - val_accuracy: 0.8694\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.87313\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3405 - accuracy: 0.8874 - val_loss: 0.6140 - val_accuracy: 0.8719\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.87313\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3219 - accuracy: 0.8908 - val_loss: 0.5976 - val_accuracy: 0.8706\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.87313\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3163 - accuracy: 0.8980 - val_loss: 0.5695 - val_accuracy: 0.8682\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.87313\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3048 - accuracy: 0.8952 - val_loss: 0.5522 - val_accuracy: 0.8719\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.87313\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 2s 12ms/step - loss: 0.2766 - accuracy: 0.9039 - val_loss: 0.5603 - val_accuracy: 0.8694\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87313\n",
      "Epoch 21/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2666 - accuracy: 0.9104 - val_loss: 0.5428 - val_accuracy: 0.8719\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.87313\n",
      "Epoch 22/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2548 - accuracy: 0.9120 - val_loss: 0.5398 - val_accuracy: 0.8706\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.87313\n",
      "Epoch 23/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2374 - accuracy: 0.9154 - val_loss: 0.5687 - val_accuracy: 0.8719\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.87313\n",
      "Epoch 24/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2146 - accuracy: 0.9260 - val_loss: 0.5606 - val_accuracy: 0.8719\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.87313\n",
      "Epoch 25/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2240 - accuracy: 0.9226 - val_loss: 0.5567 - val_accuracy: 0.8719\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.87313\n",
      "Epoch 26/50\n",
      "201/201 [==============================] - 3s 13ms/step - loss: 0.2076 - accuracy: 0.9244 - val_loss: 0.5871 - val_accuracy: 0.8669\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.87313\n",
      "Epoch 27/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2050 - accuracy: 0.9263 - val_loss: 0.5726 - val_accuracy: 0.8632\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.87313\n",
      "Epoch 28/50\n",
      "201/201 [==============================] - 3s 13ms/step - loss: 0.1682 - accuracy: 0.9400 - val_loss: 0.5937 - val_accuracy: 0.8632\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.87313\n",
      "Epoch 29/50\n",
      "201/201 [==============================] - 3s 14ms/step - loss: 0.1658 - accuracy: 0.9403 - val_loss: 0.5901 - val_accuracy: 0.8607\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.87313\n",
      "Epoch 30/50\n",
      "201/201 [==============================] - 3s 15ms/step - loss: 0.1585 - accuracy: 0.9431 - val_loss: 0.5878 - val_accuracy: 0.8669\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.87313\n",
      "Epoch 31/50\n",
      "201/201 [==============================] - 3s 15ms/step - loss: 0.1516 - accuracy: 0.9403 - val_loss: 0.6007 - val_accuracy: 0.8682\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.87313\n",
      "Epoch 32/50\n",
      "201/201 [==============================] - 2s 12ms/step - loss: 0.1584 - accuracy: 0.9440 - val_loss: 0.6458 - val_accuracy: 0.8644\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.87313\n",
      "Epoch 33/50\n",
      "201/201 [==============================] - 2s 12ms/step - loss: 0.1232 - accuracy: 0.9555 - val_loss: 0.6155 - val_accuracy: 0.8644\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.87313\n",
      "Epoch 34/50\n",
      "201/201 [==============================] - 2s 12ms/step - loss: 0.1275 - accuracy: 0.9580 - val_loss: 0.6906 - val_accuracy: 0.8694\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.87313\n",
      "Epoch 35/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1234 - accuracy: 0.9549 - val_loss: 0.6866 - val_accuracy: 0.8582\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.87313\n",
      "Epoch 36/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1225 - accuracy: 0.9540 - val_loss: 0.5952 - val_accuracy: 0.8607\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.87313\n",
      "Epoch 37/50\n",
      "201/201 [==============================] - 3s 14ms/step - loss: 0.1413 - accuracy: 0.9515 - val_loss: 0.5982 - val_accuracy: 0.8632\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.87313\n",
      "Epoch 38/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1073 - accuracy: 0.9645 - val_loss: 0.6900 - val_accuracy: 0.8632\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.87313\n",
      "Epoch 39/50\n",
      "201/201 [==============================] - 3s 13ms/step - loss: 0.1054 - accuracy: 0.9655 - val_loss: 0.6392 - val_accuracy: 0.8632\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.87313\n",
      "Epoch 40/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1014 - accuracy: 0.9652 - val_loss: 0.6950 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.87313\n",
      "Epoch 41/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.0955 - accuracy: 0.9661 - val_loss: 0.6160 - val_accuracy: 0.8607\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.87313\n",
      "Epoch 42/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.1003 - accuracy: 0.9633 - val_loss: 0.6714 - val_accuracy: 0.8607\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.87313\n",
      "Epoch 43/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.1044 - accuracy: 0.9624 - val_loss: 0.6966 - val_accuracy: 0.8557\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.87313\n",
      "Epoch 44/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.0873 - accuracy: 0.9714 - val_loss: 0.6876 - val_accuracy: 0.8607\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.87313\n",
      "Epoch 45/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.0822 - accuracy: 0.9751 - val_loss: 0.6941 - val_accuracy: 0.8669\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.87313\n",
      "Epoch 46/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.0831 - accuracy: 0.9729 - val_loss: 0.7384 - val_accuracy: 0.8607\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.87313\n",
      "Epoch 47/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.0918 - accuracy: 0.9689 - val_loss: 0.7086 - val_accuracy: 0.8607\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.87313\n",
      "Epoch 00047: early stopping\n",
      "Epoch 1/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6994 - accuracy: 0.8180 - val_loss: 0.8307 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.87313\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6553 - accuracy: 0.8202 - val_loss: 0.7421 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.87313\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6364 - accuracy: 0.8212 - val_loss: 0.7483 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.87313\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6298 - accuracy: 0.8208 - val_loss: 0.6989 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.87313\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6156 - accuracy: 0.8212 - val_loss: 0.7692 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.87313\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6058 - accuracy: 0.8212 - val_loss: 0.7094 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.87313\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5943 - accuracy: 0.8205 - val_loss: 0.6965 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.87313\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5878 - accuracy: 0.8230 - val_loss: 0.7004 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.87313\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5677 - accuracy: 0.8252 - val_loss: 0.6926 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.87313\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5658 - accuracy: 0.8249 - val_loss: 0.7025 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.87313\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5378 - accuracy: 0.8286 - val_loss: 0.7165 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.87313\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5300 - accuracy: 0.8274 - val_loss: 0.6980 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.87313\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5192 - accuracy: 0.8314 - val_loss: 0.6586 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.87313\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.4922 - accuracy: 0.8370 - val_loss: 0.6829 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.87313\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.4800 - accuracy: 0.8370 - val_loss: 0.6513 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.87313\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 2s 12ms/step - loss: 0.4713 - accuracy: 0.8407 - val_loss: 0.6589 - val_accuracy: 0.8184\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.87313\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 2s 12ms/step - loss: 0.4587 - accuracy: 0.8404 - val_loss: 0.6735 - val_accuracy: 0.8197\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.87313\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 3s 13ms/step - loss: 0.4407 - accuracy: 0.8510 - val_loss: 0.6977 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.87313\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.4060 - accuracy: 0.8566 - val_loss: 0.6816 - val_accuracy: 0.8184\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.87313\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.4098 - accuracy: 0.8554 - val_loss: 0.6919 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87313\n",
      "Epoch 21/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3839 - accuracy: 0.8597 - val_loss: 0.6874 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.87313\n",
      "Epoch 22/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3777 - accuracy: 0.8653 - val_loss: 0.6613 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.87313\n",
      "Epoch 23/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3844 - accuracy: 0.8678 - val_loss: 0.6816 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.87313\n",
      "Epoch 24/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3601 - accuracy: 0.8740 - val_loss: 0.6955 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.87313\n",
      "Epoch 25/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3466 - accuracy: 0.8774 - val_loss: 0.6941 - val_accuracy: 0.8197\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.87313\n",
      "Epoch 26/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3348 - accuracy: 0.8759 - val_loss: 0.6808 - val_accuracy: 0.8184\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.87313\n",
      "Epoch 27/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3002 - accuracy: 0.8914 - val_loss: 0.6936 - val_accuracy: 0.8197\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.87313\n",
      "Epoch 28/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2998 - accuracy: 0.8933 - val_loss: 0.7311 - val_accuracy: 0.8197\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.87313\n",
      "Epoch 29/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3051 - accuracy: 0.8921 - val_loss: 0.7672 - val_accuracy: 0.8197\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.87313\n",
      "Epoch 30/50\n",
      "201/201 [==============================] - 2s 12ms/step - loss: 0.2733 - accuracy: 0.8995 - val_loss: 0.7401 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.87313\n",
      "Epoch 31/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2793 - accuracy: 0.9008 - val_loss: 0.7442 - val_accuracy: 0.8184\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.87313\n",
      "Epoch 32/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2599 - accuracy: 0.9054 - val_loss: 0.7466 - val_accuracy: 0.8134\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.87313\n",
      "Epoch 33/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2486 - accuracy: 0.9061 - val_loss: 0.7719 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.87313\n",
      "Epoch 34/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2444 - accuracy: 0.9070 - val_loss: 0.7466 - val_accuracy: 0.8109\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.87313\n",
      "Epoch 35/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2493 - accuracy: 0.9101 - val_loss: 0.7877 - val_accuracy: 0.8109\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.87313\n",
      "Epoch 36/50\n",
      "201/201 [==============================] - 3s 13ms/step - loss: 0.2193 - accuracy: 0.9188 - val_loss: 0.7801 - val_accuracy: 0.8134\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.87313\n",
      "Epoch 37/50\n",
      "201/201 [==============================] - 2s 12ms/step - loss: 0.2196 - accuracy: 0.9201 - val_loss: 0.7801 - val_accuracy: 0.8122\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.87313\n",
      "Epoch 38/50\n",
      "201/201 [==============================] - 3s 15ms/step - loss: 0.2117 - accuracy: 0.9210 - val_loss: 0.8166 - val_accuracy: 0.8134\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.87313\n",
      "Epoch 39/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2213 - accuracy: 0.9213 - val_loss: 0.8107 - val_accuracy: 0.8035\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.87313\n",
      "Epoch 40/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2112 - accuracy: 0.9210 - val_loss: 0.7528 - val_accuracy: 0.8085\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.87313\n",
      "Epoch 00040: early stopping\n",
      "Epoch 1/50\n",
      "201/201 [==============================] - 2s 12ms/step - loss: 0.6842 - accuracy: 0.8093 - val_loss: 0.7078 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.87313\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6458 - accuracy: 0.8134 - val_loss: 0.7171 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.87313\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6299 - accuracy: 0.8134 - val_loss: 0.6861 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.87313\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 2s 12ms/step - loss: 0.6304 - accuracy: 0.8140 - val_loss: 0.7226 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.87313\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6356 - accuracy: 0.8131 - val_loss: 0.7076 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.87313\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6121 - accuracy: 0.8137 - val_loss: 0.7553 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.87313\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6008 - accuracy: 0.8140 - val_loss: 0.7169 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.87313\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6005 - accuracy: 0.8143 - val_loss: 0.6655 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.87313\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5896 - accuracy: 0.8149 - val_loss: 0.6823 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.87313\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5830 - accuracy: 0.8140 - val_loss: 0.6762 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.87313\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5716 - accuracy: 0.8171 - val_loss: 0.6488 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.87313\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5660 - accuracy: 0.8171 - val_loss: 0.6858 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.87313\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5337 - accuracy: 0.8224 - val_loss: 0.6670 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.87313\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5317 - accuracy: 0.8202 - val_loss: 0.6739 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.87313\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.5120 - accuracy: 0.8249 - val_loss: 0.6626 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.87313\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 2s 12ms/step - loss: 0.5150 - accuracy: 0.8274 - val_loss: 0.6574 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.87313\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.4978 - accuracy: 0.8277 - val_loss: 0.6821 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.87313\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4754 - accuracy: 0.8364 - val_loss: 0.6701 - val_accuracy: 0.8184\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.87313\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4678 - accuracy: 0.8361 - val_loss: 0.6597 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.87313\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.4590 - accuracy: 0.8432 - val_loss: 0.6604 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87313\n",
      "Epoch 21/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4361 - accuracy: 0.8445 - val_loss: 0.6819 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.87313\n",
      "Epoch 22/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4260 - accuracy: 0.8507 - val_loss: 0.6582 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.87313\n",
      "Epoch 23/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3902 - accuracy: 0.8526 - val_loss: 0.6634 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.87313\n",
      "Epoch 24/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3827 - accuracy: 0.8575 - val_loss: 0.6745 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.87313\n",
      "Epoch 25/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3741 - accuracy: 0.8663 - val_loss: 0.6555 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.87313\n",
      "Epoch 26/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3598 - accuracy: 0.8687 - val_loss: 0.6730 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.87313\n",
      "Epoch 27/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3543 - accuracy: 0.8737 - val_loss: 0.7038 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.87313\n",
      "Epoch 28/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3358 - accuracy: 0.8728 - val_loss: 0.6723 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.87313\n",
      "Epoch 29/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3352 - accuracy: 0.8747 - val_loss: 0.6924 - val_accuracy: 0.8109\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.87313\n",
      "Epoch 30/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2866 - accuracy: 0.8998 - val_loss: 0.7087 - val_accuracy: 0.8109\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.87313\n",
      "Epoch 31/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2839 - accuracy: 0.8936 - val_loss: 0.6831 - val_accuracy: 0.8122\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.87313\n",
      "Epoch 32/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2878 - accuracy: 0.8924 - val_loss: 0.6942 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.87313\n",
      "Epoch 33/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2743 - accuracy: 0.8977 - val_loss: 0.7023 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.87313\n",
      "Epoch 34/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2431 - accuracy: 0.9086 - val_loss: 0.7382 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.87313\n",
      "Epoch 35/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2382 - accuracy: 0.9079 - val_loss: 0.7045 - val_accuracy: 0.8060\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.87313\n",
      "Epoch 36/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.2559 - accuracy: 0.9039 - val_loss: 0.7283 - val_accuracy: 0.8085\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.87313\n",
      "Epoch 00036: early stopping\n",
      "Epoch 1/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.7592 - accuracy: 0.7723 - val_loss: 0.8209 - val_accuracy: 0.7649\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.87313\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 2s 12ms/step - loss: 0.7259 - accuracy: 0.7736 - val_loss: 0.8063 - val_accuracy: 0.7649\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.87313\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.7221 - accuracy: 0.7751 - val_loss: 0.7960 - val_accuracy: 0.7649\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.87313\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.7079 - accuracy: 0.7748 - val_loss: 0.7964 - val_accuracy: 0.7649\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.87313\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.7084 - accuracy: 0.7757 - val_loss: 0.7948 - val_accuracy: 0.7649\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.87313\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6902 - accuracy: 0.7748 - val_loss: 0.8152 - val_accuracy: 0.7649\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.87313\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6871 - accuracy: 0.7754 - val_loss: 0.7852 - val_accuracy: 0.7649\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.87313\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6770 - accuracy: 0.7776 - val_loss: 0.7880 - val_accuracy: 0.7649\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.87313\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6685 - accuracy: 0.7776 - val_loss: 0.7937 - val_accuracy: 0.7649\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.87313\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6489 - accuracy: 0.7788 - val_loss: 0.7712 - val_accuracy: 0.7649\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.87313\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6483 - accuracy: 0.7776 - val_loss: 0.7806 - val_accuracy: 0.7649\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.87313\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6402 - accuracy: 0.7823 - val_loss: 0.7882 - val_accuracy: 0.7649\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.87313\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6282 - accuracy: 0.7851 - val_loss: 0.7818 - val_accuracy: 0.7649\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.87313\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.6075 - accuracy: 0.7854 - val_loss: 0.7904 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.87313\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6034 - accuracy: 0.7876 - val_loss: 0.7679 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.87313\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5816 - accuracy: 0.7907 - val_loss: 0.7829 - val_accuracy: 0.7649\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.87313\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5700 - accuracy: 0.7960 - val_loss: 0.7869 - val_accuracy: 0.7662\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.87313\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5488 - accuracy: 0.8009 - val_loss: 0.7932 - val_accuracy: 0.7624\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.87313\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5421 - accuracy: 0.7988 - val_loss: 0.7888 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.87313\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5243 - accuracy: 0.8081 - val_loss: 0.7884 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87313\n",
      "Epoch 21/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5236 - accuracy: 0.8103 - val_loss: 0.7933 - val_accuracy: 0.7587\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.87313\n",
      "Epoch 22/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4917 - accuracy: 0.8215 - val_loss: 0.7894 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.87313\n",
      "Epoch 23/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4786 - accuracy: 0.8249 - val_loss: 0.8049 - val_accuracy: 0.7562\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.87313\n",
      "Epoch 24/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4647 - accuracy: 0.8292 - val_loss: 0.8074 - val_accuracy: 0.7575\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.87313\n",
      "Epoch 25/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4617 - accuracy: 0.8271 - val_loss: 0.8044 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.87313\n",
      "Epoch 26/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.4321 - accuracy: 0.8454 - val_loss: 0.8112 - val_accuracy: 0.7587\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.87313\n",
      "Epoch 27/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4103 - accuracy: 0.8485 - val_loss: 0.8166 - val_accuracy: 0.7488\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.87313\n",
      "Epoch 28/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3997 - accuracy: 0.8510 - val_loss: 0.8089 - val_accuracy: 0.7562\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.87313\n",
      "Epoch 29/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.4107 - accuracy: 0.8463 - val_loss: 0.8080 - val_accuracy: 0.7587\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.87313\n",
      "Epoch 30/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3905 - accuracy: 0.8529 - val_loss: 0.8063 - val_accuracy: 0.7612\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.87313\n",
      "Epoch 31/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3672 - accuracy: 0.8619 - val_loss: 0.8408 - val_accuracy: 0.7512\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.87313\n",
      "Epoch 32/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3515 - accuracy: 0.8709 - val_loss: 0.8383 - val_accuracy: 0.7537\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.87313\n",
      "Epoch 33/50\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 0.3435 - accuracy: 0.8672 - val_loss: 0.8553 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.87313\n",
      "Epoch 34/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3348 - accuracy: 0.8747 - val_loss: 0.8316 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.87313\n",
      "Epoch 35/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3341 - accuracy: 0.8812 - val_loss: 0.8739 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.87313\n",
      "Epoch 36/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3170 - accuracy: 0.8840 - val_loss: 0.8681 - val_accuracy: 0.7475\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.87313\n",
      "Epoch 37/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3028 - accuracy: 0.8830 - val_loss: 0.8768 - val_accuracy: 0.7488\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.87313\n",
      "Epoch 38/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2836 - accuracy: 0.8902 - val_loss: 0.8922 - val_accuracy: 0.7376\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.87313\n",
      "Epoch 39/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2937 - accuracy: 0.8918 - val_loss: 0.8505 - val_accuracy: 0.7475\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.87313\n",
      "Epoch 40/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2819 - accuracy: 0.8930 - val_loss: 0.9251 - val_accuracy: 0.7475\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.87313\n",
      "Epoch 00040: early stopping\n",
      "Epoch 1/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6941 - accuracy: 0.8090 - val_loss: 0.7181 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.87313\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6461 - accuracy: 0.8140 - val_loss: 0.7030 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.87313\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6353 - accuracy: 0.8146 - val_loss: 0.7236 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.87313\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6261 - accuracy: 0.8146 - val_loss: 0.7212 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.87313\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6214 - accuracy: 0.8152 - val_loss: 0.7320 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.87313\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6206 - accuracy: 0.8143 - val_loss: 0.7246 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.87313\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6128 - accuracy: 0.8149 - val_loss: 0.7202 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.87313\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6081 - accuracy: 0.8159 - val_loss: 0.7245 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.87313\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5949 - accuracy: 0.8165 - val_loss: 0.7299 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.87313\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5902 - accuracy: 0.8159 - val_loss: 0.7273 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.87313\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5822 - accuracy: 0.8162 - val_loss: 0.7230 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.87313\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5777 - accuracy: 0.8156 - val_loss: 0.7316 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.87313\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5637 - accuracy: 0.8190 - val_loss: 0.7300 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.87313\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5542 - accuracy: 0.8208 - val_loss: 0.7340 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.87313\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5323 - accuracy: 0.8212 - val_loss: 0.7230 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.87313\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5215 - accuracy: 0.8274 - val_loss: 0.7426 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.87313\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5180 - accuracy: 0.8252 - val_loss: 0.7078 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.87313\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5045 - accuracy: 0.8289 - val_loss: 0.7335 - val_accuracy: 0.8072\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.87313\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4876 - accuracy: 0.8267 - val_loss: 0.7278 - val_accuracy: 0.8072\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.87313\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4702 - accuracy: 0.8373 - val_loss: 0.7730 - val_accuracy: 0.8047\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87313\n",
      "Epoch 21/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4627 - accuracy: 0.8435 - val_loss: 0.7681 - val_accuracy: 0.8022\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.87313\n",
      "Epoch 22/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4495 - accuracy: 0.8423 - val_loss: 0.7467 - val_accuracy: 0.8060\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.87313\n",
      "Epoch 23/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4252 - accuracy: 0.8516 - val_loss: 0.7423 - val_accuracy: 0.8072\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.87313\n",
      "Epoch 24/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4271 - accuracy: 0.8516 - val_loss: 0.7553 - val_accuracy: 0.8060\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.87313\n",
      "Epoch 25/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3929 - accuracy: 0.8579 - val_loss: 0.7354 - val_accuracy: 0.8072\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.87313\n",
      "Epoch 26/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3876 - accuracy: 0.8591 - val_loss: 0.7592 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.87313\n",
      "Epoch 27/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3776 - accuracy: 0.8669 - val_loss: 0.7413 - val_accuracy: 0.8047\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.87313\n",
      "Epoch 00027: early stopping\n",
      "Epoch 1/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5872 - accuracy: 0.8392 - val_loss: 0.6480 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.87313\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5766 - accuracy: 0.8395 - val_loss: 0.6800 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.87313\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5649 - accuracy: 0.8395 - val_loss: 0.6470 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.87313\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5626 - accuracy: 0.8395 - val_loss: 0.6676 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.87313\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5523 - accuracy: 0.8395 - val_loss: 0.6690 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.87313\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5428 - accuracy: 0.8395 - val_loss: 0.6829 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.87313\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5408 - accuracy: 0.8411 - val_loss: 0.6562 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.87313\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5310 - accuracy: 0.8407 - val_loss: 0.6610 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.87313\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5249 - accuracy: 0.8392 - val_loss: 0.6972 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.87313\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5157 - accuracy: 0.8435 - val_loss: 0.6737 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.87313\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5101 - accuracy: 0.8420 - val_loss: 0.6334 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.87313\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5004 - accuracy: 0.8445 - val_loss: 0.6849 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.87313\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4889 - accuracy: 0.8432 - val_loss: 0.6764 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.87313\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4679 - accuracy: 0.8485 - val_loss: 0.6702 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.87313\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4541 - accuracy: 0.8507 - val_loss: 0.6481 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.87313\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4436 - accuracy: 0.8513 - val_loss: 0.6614 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.87313\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4357 - accuracy: 0.8526 - val_loss: 0.6434 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.87313\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4199 - accuracy: 0.8616 - val_loss: 0.6661 - val_accuracy: 0.8346\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.87313\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4064 - accuracy: 0.8631 - val_loss: 0.6570 - val_accuracy: 0.8346\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.87313\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3779 - accuracy: 0.8700 - val_loss: 0.6453 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87313\n",
      "Epoch 21/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3645 - accuracy: 0.8691 - val_loss: 0.6466 - val_accuracy: 0.8383\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.87313\n",
      "Epoch 22/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3563 - accuracy: 0.8722 - val_loss: 0.6490 - val_accuracy: 0.8371\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.87313\n",
      "Epoch 23/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3541 - accuracy: 0.8774 - val_loss: 0.6594 - val_accuracy: 0.8371\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.87313\n",
      "Epoch 24/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3435 - accuracy: 0.8771 - val_loss: 0.6591 - val_accuracy: 0.8346\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.87313\n",
      "Epoch 25/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3321 - accuracy: 0.8880 - val_loss: 0.6473 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.87313\n",
      "Epoch 26/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3011 - accuracy: 0.8899 - val_loss: 0.6566 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.87313\n",
      "Epoch 27/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3032 - accuracy: 0.8871 - val_loss: 0.6482 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.87313\n",
      "Epoch 28/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2876 - accuracy: 0.8911 - val_loss: 0.6657 - val_accuracy: 0.8234\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.87313\n",
      "Epoch 29/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2749 - accuracy: 0.8983 - val_loss: 0.6549 - val_accuracy: 0.8296\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.87313\n",
      "Epoch 30/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2622 - accuracy: 0.9039 - val_loss: 0.6716 - val_accuracy: 0.8296\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.87313\n",
      "Epoch 31/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2397 - accuracy: 0.9151 - val_loss: 0.6586 - val_accuracy: 0.8308\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.87313\n",
      "Epoch 32/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2484 - accuracy: 0.9089 - val_loss: 0.6708 - val_accuracy: 0.8246\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.87313\n",
      "Epoch 33/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2394 - accuracy: 0.9129 - val_loss: 0.6603 - val_accuracy: 0.8259\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.87313\n",
      "Epoch 34/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2148 - accuracy: 0.9185 - val_loss: 0.7004 - val_accuracy: 0.8184\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.87313\n",
      "Epoch 35/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2177 - accuracy: 0.9213 - val_loss: 0.6910 - val_accuracy: 0.8234\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.87313\n",
      "Epoch 36/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2282 - accuracy: 0.9173 - val_loss: 0.6745 - val_accuracy: 0.8234\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.87313\n",
      "Epoch 00036: early stopping\n",
      "Epoch 1/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5717 - accuracy: 0.8467 - val_loss: 0.6190 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.87313\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5578 - accuracy: 0.8473 - val_loss: 0.5815 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.87313\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5431 - accuracy: 0.8476 - val_loss: 0.6172 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.87313\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5390 - accuracy: 0.8476 - val_loss: 0.5979 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.87313\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5309 - accuracy: 0.8479 - val_loss: 0.5808 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.87313\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5335 - accuracy: 0.8482 - val_loss: 0.6179 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.87313\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5267 - accuracy: 0.8476 - val_loss: 0.5873 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.87313\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5097 - accuracy: 0.8479 - val_loss: 0.6008 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.87313\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5076 - accuracy: 0.8479 - val_loss: 0.5975 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.87313\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4986 - accuracy: 0.8491 - val_loss: 0.6029 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.87313\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4963 - accuracy: 0.8507 - val_loss: 0.5831 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.87313\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4863 - accuracy: 0.8516 - val_loss: 0.6026 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.87313\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4768 - accuracy: 0.8523 - val_loss: 0.5903 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.87313\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4616 - accuracy: 0.8532 - val_loss: 0.6160 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.87313\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4483 - accuracy: 0.8563 - val_loss: 0.6117 - val_accuracy: 0.8607\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.87313\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4428 - accuracy: 0.8569 - val_loss: 0.5891 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.87313\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4235 - accuracy: 0.8582 - val_loss: 0.6091 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.87313\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4087 - accuracy: 0.8635 - val_loss: 0.5701 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.87313\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3995 - accuracy: 0.8625 - val_loss: 0.5840 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.87313\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3967 - accuracy: 0.8675 - val_loss: 0.5773 - val_accuracy: 0.8607\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87313\n",
      "Epoch 21/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3889 - accuracy: 0.8691 - val_loss: 0.5855 - val_accuracy: 0.8607\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.87313\n",
      "Epoch 22/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3697 - accuracy: 0.8768 - val_loss: 0.5933 - val_accuracy: 0.8582\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.87313\n",
      "Epoch 23/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3527 - accuracy: 0.8762 - val_loss: 0.6029 - val_accuracy: 0.8607\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.87313\n",
      "Epoch 24/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3384 - accuracy: 0.8809 - val_loss: 0.5844 - val_accuracy: 0.8607\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.87313\n",
      "Epoch 25/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3384 - accuracy: 0.8846 - val_loss: 0.5675 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.87313\n",
      "Epoch 26/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3199 - accuracy: 0.8840 - val_loss: 0.6238 - val_accuracy: 0.8545\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.87313\n",
      "Epoch 27/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3111 - accuracy: 0.8949 - val_loss: 0.5718 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.87313\n",
      "Epoch 28/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2951 - accuracy: 0.8986 - val_loss: 0.5772 - val_accuracy: 0.8570\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.87313\n",
      "Epoch 29/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2962 - accuracy: 0.8939 - val_loss: 0.6039 - val_accuracy: 0.8545\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.87313\n",
      "Epoch 30/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2795 - accuracy: 0.8977 - val_loss: 0.5909 - val_accuracy: 0.8582\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.87313\n",
      "Epoch 31/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2589 - accuracy: 0.9086 - val_loss: 0.6251 - val_accuracy: 0.8507\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.87313\n",
      "Epoch 32/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2473 - accuracy: 0.9092 - val_loss: 0.6150 - val_accuracy: 0.8507\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.87313\n",
      "Epoch 33/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2427 - accuracy: 0.9163 - val_loss: 0.6043 - val_accuracy: 0.8545\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.87313\n",
      "Epoch 34/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2354 - accuracy: 0.9157 - val_loss: 0.5730 - val_accuracy: 0.8570\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.87313\n",
      "Epoch 35/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2185 - accuracy: 0.9210 - val_loss: 0.6082 - val_accuracy: 0.8532\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.87313\n",
      "Epoch 36/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2056 - accuracy: 0.9269 - val_loss: 0.6300 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.87313\n",
      "Epoch 37/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2181 - accuracy: 0.9222 - val_loss: 0.5930 - val_accuracy: 0.8582\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.87313\n",
      "Epoch 38/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2033 - accuracy: 0.9263 - val_loss: 0.6286 - val_accuracy: 0.8433\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.87313\n",
      "Epoch 39/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.1942 - accuracy: 0.9319 - val_loss: 0.6212 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.87313\n",
      "Epoch 40/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.1947 - accuracy: 0.9288 - val_loss: 0.5890 - val_accuracy: 0.8545\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.87313\n",
      "Epoch 41/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.1739 - accuracy: 0.9365 - val_loss: 0.6034 - val_accuracy: 0.8545\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.87313\n",
      "Epoch 42/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.1740 - accuracy: 0.9365 - val_loss: 0.6067 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.87313\n",
      "Epoch 43/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.1533 - accuracy: 0.9484 - val_loss: 0.6156 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.87313\n",
      "Epoch 44/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.1702 - accuracy: 0.9347 - val_loss: 0.6327 - val_accuracy: 0.8433\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.87313\n",
      "Epoch 45/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.1632 - accuracy: 0.9406 - val_loss: 0.5955 - val_accuracy: 0.8532\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.87313\n",
      "Epoch 46/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.1456 - accuracy: 0.9471 - val_loss: 0.6557 - val_accuracy: 0.8507\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.87313\n",
      "Epoch 47/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.1579 - accuracy: 0.9449 - val_loss: 0.6634 - val_accuracy: 0.8408\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.87313\n",
      "Epoch 48/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.1398 - accuracy: 0.9518 - val_loss: 0.6386 - val_accuracy: 0.8408\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.87313\n",
      "Epoch 49/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.1467 - accuracy: 0.9474 - val_loss: 0.6414 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.87313\n",
      "Epoch 50/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.1277 - accuracy: 0.9537 - val_loss: 0.6230 - val_accuracy: 0.8483\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.87313\n",
      "Epoch 00050: early stopping\n",
      "Epoch 1/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6960 - accuracy: 0.8065 - val_loss: 0.7247 - val_accuracy: 0.7973\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.87313\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6440 - accuracy: 0.8115 - val_loss: 0.7041 - val_accuracy: 0.7973\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.87313\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6295 - accuracy: 0.8124 - val_loss: 0.7146 - val_accuracy: 0.7973\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.87313\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6233 - accuracy: 0.8124 - val_loss: 0.7099 - val_accuracy: 0.7973\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.87313\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6177 - accuracy: 0.8140 - val_loss: 0.7215 - val_accuracy: 0.7973\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.87313\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6146 - accuracy: 0.8121 - val_loss: 0.7187 - val_accuracy: 0.7973\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.87313\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6093 - accuracy: 0.8118 - val_loss: 0.6954 - val_accuracy: 0.7973\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.87313\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6058 - accuracy: 0.8137 - val_loss: 0.7222 - val_accuracy: 0.7973\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.87313\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6063 - accuracy: 0.8146 - val_loss: 0.7014 - val_accuracy: 0.7973\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.87313\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5973 - accuracy: 0.8124 - val_loss: 0.7043 - val_accuracy: 0.7973\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.87313\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5860 - accuracy: 0.8140 - val_loss: 0.7097 - val_accuracy: 0.7973\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.87313\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5794 - accuracy: 0.8162 - val_loss: 0.7026 - val_accuracy: 0.7973\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.87313\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5748 - accuracy: 0.8159 - val_loss: 0.6843 - val_accuracy: 0.7973\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.87313\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5729 - accuracy: 0.8162 - val_loss: 0.6890 - val_accuracy: 0.7973\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.87313\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5534 - accuracy: 0.8187 - val_loss: 0.6985 - val_accuracy: 0.7973\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.87313\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5576 - accuracy: 0.8193 - val_loss: 0.6996 - val_accuracy: 0.7960\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.87313\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5466 - accuracy: 0.8187 - val_loss: 0.6909 - val_accuracy: 0.7973\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.87313\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5349 - accuracy: 0.8205 - val_loss: 0.7078 - val_accuracy: 0.7973\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.87313\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5332 - accuracy: 0.8221 - val_loss: 0.6957 - val_accuracy: 0.7923\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.87313\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5177 - accuracy: 0.8280 - val_loss: 0.6830 - val_accuracy: 0.7960\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87313\n",
      "Epoch 21/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5057 - accuracy: 0.8280 - val_loss: 0.6957 - val_accuracy: 0.7935\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.87313\n",
      "Epoch 22/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4916 - accuracy: 0.8323 - val_loss: 0.7179 - val_accuracy: 0.7935\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.87313\n",
      "Epoch 23/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4886 - accuracy: 0.8320 - val_loss: 0.7084 - val_accuracy: 0.7886\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.87313\n",
      "Epoch 24/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4631 - accuracy: 0.8448 - val_loss: 0.6981 - val_accuracy: 0.7910\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.87313\n",
      "Epoch 25/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4654 - accuracy: 0.8389 - val_loss: 0.7065 - val_accuracy: 0.7923\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.87313\n",
      "Epoch 26/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4481 - accuracy: 0.8407 - val_loss: 0.6877 - val_accuracy: 0.7935\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.87313\n",
      "Epoch 27/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4354 - accuracy: 0.8426 - val_loss: 0.7090 - val_accuracy: 0.7898\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.87313\n",
      "Epoch 28/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4163 - accuracy: 0.8529 - val_loss: 0.7037 - val_accuracy: 0.7873\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.87313\n",
      "Epoch 29/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4142 - accuracy: 0.8594 - val_loss: 0.6948 - val_accuracy: 0.7923\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.87313\n",
      "Epoch 30/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4055 - accuracy: 0.8603 - val_loss: 0.6978 - val_accuracy: 0.7923\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.87313\n",
      "Epoch 31/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4071 - accuracy: 0.8591 - val_loss: 0.6948 - val_accuracy: 0.7923\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.87313\n",
      "Epoch 32/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3759 - accuracy: 0.8622 - val_loss: 0.7141 - val_accuracy: 0.7886\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.87313\n",
      "Epoch 33/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3777 - accuracy: 0.8656 - val_loss: 0.7014 - val_accuracy: 0.7898\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.87313\n",
      "Epoch 34/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3659 - accuracy: 0.8715 - val_loss: 0.7068 - val_accuracy: 0.7848\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.87313\n",
      "Epoch 35/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3604 - accuracy: 0.8712 - val_loss: 0.6854 - val_accuracy: 0.7935\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.87313\n",
      "Epoch 36/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3219 - accuracy: 0.8855 - val_loss: 0.7077 - val_accuracy: 0.7886\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.87313\n",
      "Epoch 37/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3268 - accuracy: 0.8796 - val_loss: 0.7005 - val_accuracy: 0.7861\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.87313\n",
      "Epoch 38/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3309 - accuracy: 0.8771 - val_loss: 0.6977 - val_accuracy: 0.7935\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.87313\n",
      "Epoch 39/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3356 - accuracy: 0.8790 - val_loss: 0.7022 - val_accuracy: 0.7923\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.87313\n",
      "Epoch 40/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3070 - accuracy: 0.8902 - val_loss: 0.7016 - val_accuracy: 0.7836\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.87313\n",
      "Epoch 41/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3035 - accuracy: 0.8886 - val_loss: 0.6947 - val_accuracy: 0.7873\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.87313\n",
      "Epoch 42/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2937 - accuracy: 0.8936 - val_loss: 0.7392 - val_accuracy: 0.7649\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.87313\n",
      "Epoch 43/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2902 - accuracy: 0.8961 - val_loss: 0.7179 - val_accuracy: 0.7861\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.87313\n",
      "Epoch 44/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2821 - accuracy: 0.8918 - val_loss: 0.7214 - val_accuracy: 0.7799\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.87313\n",
      "Epoch 45/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2549 - accuracy: 0.9079 - val_loss: 0.7307 - val_accuracy: 0.7799\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.87313\n",
      "Epoch 00045: early stopping\n",
      "Epoch 1/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6430 - accuracy: 0.8199 - val_loss: 0.7312 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.87313\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6184 - accuracy: 0.8230 - val_loss: 0.7344 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.87313\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5972 - accuracy: 0.8233 - val_loss: 0.7235 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.87313\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5910 - accuracy: 0.8240 - val_loss: 0.7240 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.87313\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5880 - accuracy: 0.8233 - val_loss: 0.7295 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.87313\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5859 - accuracy: 0.8221 - val_loss: 0.7317 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.87313\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5810 - accuracy: 0.8236 - val_loss: 0.7300 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.87313\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5725 - accuracy: 0.8215 - val_loss: 0.7164 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.87313\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5707 - accuracy: 0.8243 - val_loss: 0.7288 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.87313\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5648 - accuracy: 0.8227 - val_loss: 0.7256 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.87313\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5668 - accuracy: 0.8236 - val_loss: 0.7174 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.87313\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5595 - accuracy: 0.8243 - val_loss: 0.7322 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.87313\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5478 - accuracy: 0.8271 - val_loss: 0.7216 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.87313\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5374 - accuracy: 0.8286 - val_loss: 0.7235 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.87313\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5321 - accuracy: 0.8261 - val_loss: 0.7298 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.87313\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5116 - accuracy: 0.8311 - val_loss: 0.7267 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.87313\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5126 - accuracy: 0.8355 - val_loss: 0.7281 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.87313\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4924 - accuracy: 0.8339 - val_loss: 0.7287 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.87313\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4793 - accuracy: 0.8401 - val_loss: 0.7290 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.87313\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4704 - accuracy: 0.8448 - val_loss: 0.7238 - val_accuracy: 0.7799\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87313\n",
      "Epoch 21/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4665 - accuracy: 0.8389 - val_loss: 0.7332 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.87313\n",
      "Epoch 22/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4495 - accuracy: 0.8467 - val_loss: 0.7336 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.87313\n",
      "Epoch 23/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4334 - accuracy: 0.8551 - val_loss: 0.7498 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.87313\n",
      "Epoch 24/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4183 - accuracy: 0.8538 - val_loss: 0.7411 - val_accuracy: 0.7786\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.87313\n",
      "Epoch 25/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4249 - accuracy: 0.8526 - val_loss: 0.7381 - val_accuracy: 0.7774\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.87313\n",
      "Epoch 26/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4184 - accuracy: 0.8526 - val_loss: 0.7355 - val_accuracy: 0.7799\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.87313\n",
      "Epoch 27/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4019 - accuracy: 0.8591 - val_loss: 0.7319 - val_accuracy: 0.7749\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.87313\n",
      "Epoch 28/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3821 - accuracy: 0.8653 - val_loss: 0.7469 - val_accuracy: 0.7761\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.87313\n",
      "Epoch 29/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3844 - accuracy: 0.8641 - val_loss: 0.7311 - val_accuracy: 0.7761\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.87313\n",
      "Epoch 30/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3619 - accuracy: 0.8728 - val_loss: 0.7401 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.87313\n",
      "Epoch 31/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3559 - accuracy: 0.8725 - val_loss: 0.7539 - val_accuracy: 0.7699\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.87313\n",
      "Epoch 32/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3418 - accuracy: 0.8809 - val_loss: 0.7413 - val_accuracy: 0.7774\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.87313\n",
      "Epoch 33/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3485 - accuracy: 0.8762 - val_loss: 0.7436 - val_accuracy: 0.7736\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.87313\n",
      "Epoch 00033: early stopping\n",
      "Epoch 1/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6460 - accuracy: 0.8156 - val_loss: 0.6517 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.87313\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6330 - accuracy: 0.8159 - val_loss: 0.6437 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.87313\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6308 - accuracy: 0.8165 - val_loss: 0.6488 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.87313\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6127 - accuracy: 0.8159 - val_loss: 0.6210 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.87313\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6145 - accuracy: 0.8159 - val_loss: 0.6379 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.87313\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6082 - accuracy: 0.8162 - val_loss: 0.6517 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.87313\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6100 - accuracy: 0.8152 - val_loss: 0.6335 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.87313\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6115 - accuracy: 0.8171 - val_loss: 0.6403 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.87313\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6066 - accuracy: 0.8162 - val_loss: 0.6366 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.87313\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5951 - accuracy: 0.8162 - val_loss: 0.6120 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.87313\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5957 - accuracy: 0.8171 - val_loss: 0.6247 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.87313\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5908 - accuracy: 0.8165 - val_loss: 0.6275 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.87313\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5800 - accuracy: 0.8171 - val_loss: 0.6135 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.87313\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5698 - accuracy: 0.8196 - val_loss: 0.6295 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.87313\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5657 - accuracy: 0.8193 - val_loss: 0.6420 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.87313\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5544 - accuracy: 0.8187 - val_loss: 0.6172 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.87313\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5626 - accuracy: 0.8187 - val_loss: 0.6166 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.87313\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5429 - accuracy: 0.8205 - val_loss: 0.6187 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.87313\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5326 - accuracy: 0.8252 - val_loss: 0.6323 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.87313\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5226 - accuracy: 0.8255 - val_loss: 0.6505 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87313\n",
      "Epoch 21/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5058 - accuracy: 0.8292 - val_loss: 0.6082 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.87313\n",
      "Epoch 22/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5190 - accuracy: 0.8243 - val_loss: 0.6259 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.87313\n",
      "Epoch 23/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4875 - accuracy: 0.8320 - val_loss: 0.6468 - val_accuracy: 0.8308\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.87313\n",
      "Epoch 24/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4915 - accuracy: 0.8323 - val_loss: 0.6354 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.87313\n",
      "Epoch 25/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4797 - accuracy: 0.8345 - val_loss: 0.6160 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.87313\n",
      "Epoch 26/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4646 - accuracy: 0.8364 - val_loss: 0.6317 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.87313\n",
      "Epoch 27/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4497 - accuracy: 0.8445 - val_loss: 0.6243 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.87313\n",
      "Epoch 28/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4491 - accuracy: 0.8398 - val_loss: 0.6220 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.87313\n",
      "Epoch 29/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4290 - accuracy: 0.8519 - val_loss: 0.6190 - val_accuracy: 0.8296\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.87313\n",
      "Epoch 30/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4278 - accuracy: 0.8467 - val_loss: 0.6404 - val_accuracy: 0.8284\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.87313\n",
      "Epoch 31/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4097 - accuracy: 0.8541 - val_loss: 0.6239 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.87313\n",
      "Epoch 32/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4018 - accuracy: 0.8600 - val_loss: 0.6182 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.87313\n",
      "Epoch 33/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3839 - accuracy: 0.8610 - val_loss: 0.6489 - val_accuracy: 0.8284\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.87313\n",
      "Epoch 34/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3835 - accuracy: 0.8625 - val_loss: 0.6305 - val_accuracy: 0.8308\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.87313\n",
      "Epoch 35/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3590 - accuracy: 0.8656 - val_loss: 0.6148 - val_accuracy: 0.8321\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.87313\n",
      "Epoch 36/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3574 - accuracy: 0.8684 - val_loss: 0.6557 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.87313\n",
      "Epoch 37/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3511 - accuracy: 0.8722 - val_loss: 0.6275 - val_accuracy: 0.8308\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.87313\n",
      "Epoch 38/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3419 - accuracy: 0.8756 - val_loss: 0.6380 - val_accuracy: 0.8259\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.87313\n",
      "Epoch 39/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3321 - accuracy: 0.8793 - val_loss: 0.6177 - val_accuracy: 0.8271\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.87313\n",
      "Epoch 40/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3164 - accuracy: 0.8874 - val_loss: 0.6195 - val_accuracy: 0.8271\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.87313\n",
      "Epoch 41/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2957 - accuracy: 0.8886 - val_loss: 0.6305 - val_accuracy: 0.8234\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.87313\n",
      "Epoch 42/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3122 - accuracy: 0.8871 - val_loss: 0.6347 - val_accuracy: 0.8284\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.87313\n",
      "Epoch 43/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3113 - accuracy: 0.8843 - val_loss: 0.6345 - val_accuracy: 0.8197\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.87313\n",
      "Epoch 44/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2800 - accuracy: 0.8974 - val_loss: 0.6376 - val_accuracy: 0.8246\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.87313\n",
      "Epoch 45/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2943 - accuracy: 0.8936 - val_loss: 0.6368 - val_accuracy: 0.8234\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.87313\n",
      "Epoch 46/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.2926 - accuracy: 0.8914 - val_loss: 0.6429 - val_accuracy: 0.8184\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.87313\n",
      "Epoch 00046: early stopping\n",
      "Epoch 1/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5810 - accuracy: 0.8457 - val_loss: 0.6369 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.87313\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5422 - accuracy: 0.8498 - val_loss: 0.6353 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.87313\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5352 - accuracy: 0.8507 - val_loss: 0.6395 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.87313\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5292 - accuracy: 0.8504 - val_loss: 0.6399 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.87313\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5226 - accuracy: 0.8519 - val_loss: 0.6480 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.87313\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5173 - accuracy: 0.8498 - val_loss: 0.6272 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.87313\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5057 - accuracy: 0.8510 - val_loss: 0.6518 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.87313\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5112 - accuracy: 0.8504 - val_loss: 0.6589 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.87313\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5008 - accuracy: 0.8504 - val_loss: 0.6580 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.87313\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5006 - accuracy: 0.8516 - val_loss: 0.6316 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.87313\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4895 - accuracy: 0.8507 - val_loss: 0.6473 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.87313\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4837 - accuracy: 0.8519 - val_loss: 0.6311 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.87313\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4756 - accuracy: 0.8513 - val_loss: 0.6427 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.87313\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4661 - accuracy: 0.8541 - val_loss: 0.6422 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.87313\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4727 - accuracy: 0.8547 - val_loss: 0.6426 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.87313\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4629 - accuracy: 0.8538 - val_loss: 0.6531 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.87313\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4524 - accuracy: 0.8554 - val_loss: 0.6429 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.87313\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4437 - accuracy: 0.8554 - val_loss: 0.6532 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.87313\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4378 - accuracy: 0.8582 - val_loss: 0.6408 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.87313\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4317 - accuracy: 0.8619 - val_loss: 0.6498 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87313\n",
      "Epoch 21/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4210 - accuracy: 0.8594 - val_loss: 0.6420 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.87313\n",
      "Epoch 22/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4158 - accuracy: 0.8622 - val_loss: 0.6509 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.87313\n",
      "Epoch 23/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4028 - accuracy: 0.8628 - val_loss: 0.6430 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.87313\n",
      "Epoch 24/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3996 - accuracy: 0.8694 - val_loss: 0.6612 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.87313\n",
      "Epoch 25/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3840 - accuracy: 0.8697 - val_loss: 0.6482 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.87313\n",
      "Epoch 26/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3678 - accuracy: 0.8715 - val_loss: 0.6587 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.87313\n",
      "Epoch 27/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3617 - accuracy: 0.8743 - val_loss: 0.6533 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.87313\n",
      "Epoch 28/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3465 - accuracy: 0.8774 - val_loss: 0.6563 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.87313\n",
      "Epoch 29/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3456 - accuracy: 0.8781 - val_loss: 0.6536 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.87313\n",
      "Epoch 30/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3399 - accuracy: 0.8771 - val_loss: 0.6436 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.87313\n",
      "Epoch 31/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3286 - accuracy: 0.8815 - val_loss: 0.6416 - val_accuracy: 0.8172\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.87313\n",
      "Epoch 00031: early stopping\n",
      "Epoch 1/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6831 - accuracy: 0.7953 - val_loss: 0.7091 - val_accuracy: 0.7923\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.87313\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6762 - accuracy: 0.7941 - val_loss: 0.7106 - val_accuracy: 0.7923\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.87313\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6655 - accuracy: 0.7947 - val_loss: 0.7063 - val_accuracy: 0.7923\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.87313\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6543 - accuracy: 0.7956 - val_loss: 0.7062 - val_accuracy: 0.7923\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.87313\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6539 - accuracy: 0.7938 - val_loss: 0.7095 - val_accuracy: 0.7923\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.87313\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6474 - accuracy: 0.7947 - val_loss: 0.7114 - val_accuracy: 0.7923\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.87313\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6435 - accuracy: 0.7956 - val_loss: 0.7001 - val_accuracy: 0.7910\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.87313\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6357 - accuracy: 0.7972 - val_loss: 0.7049 - val_accuracy: 0.7910\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.87313\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6328 - accuracy: 0.7941 - val_loss: 0.6962 - val_accuracy: 0.7910\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.87313\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6247 - accuracy: 0.7984 - val_loss: 0.6806 - val_accuracy: 0.7923\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.87313\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6185 - accuracy: 0.7988 - val_loss: 0.7277 - val_accuracy: 0.7910\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.87313\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6227 - accuracy: 0.7988 - val_loss: 0.7140 - val_accuracy: 0.7910\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.87313\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6153 - accuracy: 0.7988 - val_loss: 0.7135 - val_accuracy: 0.7910\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.87313\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5972 - accuracy: 0.8009 - val_loss: 0.7233 - val_accuracy: 0.7898\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.87313\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5907 - accuracy: 0.8009 - val_loss: 0.7149 - val_accuracy: 0.7898\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.87313\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5809 - accuracy: 0.8059 - val_loss: 0.6992 - val_accuracy: 0.7898\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.87313\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5800 - accuracy: 0.8075 - val_loss: 0.7091 - val_accuracy: 0.7886\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.87313\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5675 - accuracy: 0.8075 - val_loss: 0.7031 - val_accuracy: 0.7898\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.87313\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5570 - accuracy: 0.8128 - val_loss: 0.7238 - val_accuracy: 0.7861\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.87313\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5505 - accuracy: 0.8131 - val_loss: 0.6965 - val_accuracy: 0.7910\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87313\n",
      "Epoch 21/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5587 - accuracy: 0.8103 - val_loss: 0.7079 - val_accuracy: 0.7898\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.87313\n",
      "Epoch 22/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5369 - accuracy: 0.8103 - val_loss: 0.6993 - val_accuracy: 0.7898\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.87313\n",
      "Epoch 23/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5233 - accuracy: 0.8199 - val_loss: 0.6992 - val_accuracy: 0.7898\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.87313\n",
      "Epoch 24/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5170 - accuracy: 0.8230 - val_loss: 0.7276 - val_accuracy: 0.7873\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.87313\n",
      "Epoch 25/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5044 - accuracy: 0.8258 - val_loss: 0.7251 - val_accuracy: 0.7811\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.87313\n",
      "Epoch 26/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4992 - accuracy: 0.8193 - val_loss: 0.7157 - val_accuracy: 0.7848\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.87313\n",
      "Epoch 27/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4945 - accuracy: 0.8146 - val_loss: 0.7282 - val_accuracy: 0.7799\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.87313\n",
      "Epoch 28/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4800 - accuracy: 0.8302 - val_loss: 0.7215 - val_accuracy: 0.7799\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.87313\n",
      "Epoch 29/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4722 - accuracy: 0.8305 - val_loss: 0.6934 - val_accuracy: 0.7848\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.87313\n",
      "Epoch 30/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4600 - accuracy: 0.8361 - val_loss: 0.6938 - val_accuracy: 0.7861\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.87313\n",
      "Epoch 31/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4719 - accuracy: 0.8351 - val_loss: 0.6927 - val_accuracy: 0.7886\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.87313\n",
      "Epoch 32/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4432 - accuracy: 0.8386 - val_loss: 0.7088 - val_accuracy: 0.7873\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.87313\n",
      "Epoch 33/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4377 - accuracy: 0.8373 - val_loss: 0.7140 - val_accuracy: 0.7799\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.87313\n",
      "Epoch 34/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4300 - accuracy: 0.8423 - val_loss: 0.7100 - val_accuracy: 0.7774\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.87313\n",
      "Epoch 35/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4198 - accuracy: 0.8370 - val_loss: 0.7098 - val_accuracy: 0.7811\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.87313\n",
      "Epoch 00035: early stopping\n",
      "Epoch 1/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5843 - accuracy: 0.8414 - val_loss: 0.5861 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.87313\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5738 - accuracy: 0.8404 - val_loss: 0.5802 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.87313\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5616 - accuracy: 0.8398 - val_loss: 0.5820 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.87313\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5615 - accuracy: 0.8407 - val_loss: 0.5755 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.87313\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5521 - accuracy: 0.8411 - val_loss: 0.5684 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.87313\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5526 - accuracy: 0.8411 - val_loss: 0.5645 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.87313\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5451 - accuracy: 0.8411 - val_loss: 0.5729 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.87313\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5432 - accuracy: 0.8414 - val_loss: 0.5686 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.87313\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5376 - accuracy: 0.8417 - val_loss: 0.5593 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.87313\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5350 - accuracy: 0.8432 - val_loss: 0.5704 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.87313\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5283 - accuracy: 0.8426 - val_loss: 0.5945 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.87313\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5282 - accuracy: 0.8426 - val_loss: 0.5708 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.87313\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5155 - accuracy: 0.8445 - val_loss: 0.5788 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.87313\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5133 - accuracy: 0.8435 - val_loss: 0.5980 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.87313\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5155 - accuracy: 0.8432 - val_loss: 0.5838 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.87313\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4969 - accuracy: 0.8451 - val_loss: 0.5903 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.87313\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4950 - accuracy: 0.8485 - val_loss: 0.5857 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.87313\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4785 - accuracy: 0.8504 - val_loss: 0.5770 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.87313\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4662 - accuracy: 0.8498 - val_loss: 0.5800 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.87313\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4628 - accuracy: 0.8495 - val_loss: 0.5628 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87313\n",
      "Epoch 21/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4566 - accuracy: 0.8513 - val_loss: 0.6135 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.87313\n",
      "Epoch 22/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4442 - accuracy: 0.8575 - val_loss: 0.5877 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.87313\n",
      "Epoch 23/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4444 - accuracy: 0.8526 - val_loss: 0.5799 - val_accuracy: 0.8507\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.87313\n",
      "Epoch 24/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4337 - accuracy: 0.8560 - val_loss: 0.5867 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.87313\n",
      "Epoch 25/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4268 - accuracy: 0.8588 - val_loss: 0.5778 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.87313\n",
      "Epoch 26/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4108 - accuracy: 0.8575 - val_loss: 0.5884 - val_accuracy: 0.8470\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.87313\n",
      "Epoch 27/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4046 - accuracy: 0.8635 - val_loss: 0.5779 - val_accuracy: 0.8470\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.87313\n",
      "Epoch 28/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3962 - accuracy: 0.8647 - val_loss: 0.5827 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.87313\n",
      "Epoch 29/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3977 - accuracy: 0.8616 - val_loss: 0.5761 - val_accuracy: 0.8470\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.87313\n",
      "Epoch 30/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3762 - accuracy: 0.8731 - val_loss: 0.5811 - val_accuracy: 0.8470\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.87313\n",
      "Epoch 31/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3817 - accuracy: 0.8691 - val_loss: 0.6230 - val_accuracy: 0.8445\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.87313\n",
      "Epoch 32/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3804 - accuracy: 0.8703 - val_loss: 0.5782 - val_accuracy: 0.8470\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.87313\n",
      "Epoch 33/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3601 - accuracy: 0.8753 - val_loss: 0.5916 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.87313\n",
      "Epoch 34/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.3504 - accuracy: 0.8731 - val_loss: 0.5719 - val_accuracy: 0.8483\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.87313\n",
      "Epoch 00034: early stopping\n",
      "Epoch 1/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6580 - accuracy: 0.8084 - val_loss: 0.6672 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.87313\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6532 - accuracy: 0.8090 - val_loss: 0.6597 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.87313\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6494 - accuracy: 0.8090 - val_loss: 0.6664 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.87313\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6343 - accuracy: 0.8087 - val_loss: 0.6605 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.87313\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6286 - accuracy: 0.8100 - val_loss: 0.6610 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.87313\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6277 - accuracy: 0.8084 - val_loss: 0.6595 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.87313\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6274 - accuracy: 0.8087 - val_loss: 0.6563 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.87313\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6256 - accuracy: 0.8090 - val_loss: 0.6577 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.87313\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6205 - accuracy: 0.8078 - val_loss: 0.6617 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.87313\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6189 - accuracy: 0.8084 - val_loss: 0.6579 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.87313\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6135 - accuracy: 0.8100 - val_loss: 0.6742 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.87313\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6089 - accuracy: 0.8090 - val_loss: 0.6568 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.87313\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5953 - accuracy: 0.8090 - val_loss: 0.6724 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.87313\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5982 - accuracy: 0.8103 - val_loss: 0.6708 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.87313\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5893 - accuracy: 0.8109 - val_loss: 0.6570 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.87313\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5888 - accuracy: 0.8121 - val_loss: 0.6655 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.87313\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5823 - accuracy: 0.8112 - val_loss: 0.6641 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.87313\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5659 - accuracy: 0.8128 - val_loss: 0.6669 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.87313\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5513 - accuracy: 0.8140 - val_loss: 0.6702 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.87313\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5576 - accuracy: 0.8159 - val_loss: 0.6689 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87313\n",
      "Epoch 21/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5614 - accuracy: 0.8146 - val_loss: 0.6748 - val_accuracy: 0.8060\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.87313\n",
      "Epoch 22/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5459 - accuracy: 0.8171 - val_loss: 0.6831 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.87313\n",
      "Epoch 23/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5381 - accuracy: 0.8196 - val_loss: 0.6689 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.87313\n",
      "Epoch 24/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5270 - accuracy: 0.8184 - val_loss: 0.6668 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.87313\n",
      "Epoch 25/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5225 - accuracy: 0.8236 - val_loss: 0.6781 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.87313\n",
      "Epoch 26/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5084 - accuracy: 0.8221 - val_loss: 0.6704 - val_accuracy: 0.8072\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.87313\n",
      "Epoch 27/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5139 - accuracy: 0.8221 - val_loss: 0.6695 - val_accuracy: 0.8072\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.87313\n",
      "Epoch 28/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5002 - accuracy: 0.8302 - val_loss: 0.6809 - val_accuracy: 0.8072\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.87313\n",
      "Epoch 29/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4900 - accuracy: 0.8336 - val_loss: 0.6742 - val_accuracy: 0.8047\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.87313\n",
      "Epoch 30/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4890 - accuracy: 0.8305 - val_loss: 0.6889 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.87313\n",
      "Epoch 31/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4792 - accuracy: 0.8308 - val_loss: 0.6706 - val_accuracy: 0.8072\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.87313\n",
      "Epoch 32/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4604 - accuracy: 0.8389 - val_loss: 0.6773 - val_accuracy: 0.8072\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.87313\n",
      "Epoch 00032: early stopping\n",
      "Epoch 1/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6284 - accuracy: 0.8177 - val_loss: 0.5852 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.87313\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6232 - accuracy: 0.8171 - val_loss: 0.5745 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.87313\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6200 - accuracy: 0.8187 - val_loss: 0.5912 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.87313\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6163 - accuracy: 0.8180 - val_loss: 0.5740 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.87313\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6066 - accuracy: 0.8190 - val_loss: 0.5822 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.87313\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6094 - accuracy: 0.8177 - val_loss: 0.5864 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.87313\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6054 - accuracy: 0.8177 - val_loss: 0.6067 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.87313\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6039 - accuracy: 0.8184 - val_loss: 0.5789 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.87313\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5981 - accuracy: 0.8184 - val_loss: 0.5858 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.87313\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5977 - accuracy: 0.8193 - val_loss: 0.5610 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.87313\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5839 - accuracy: 0.8208 - val_loss: 0.5787 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.87313\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5876 - accuracy: 0.8193 - val_loss: 0.5857 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.87313\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5784 - accuracy: 0.8190 - val_loss: 0.5726 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.87313\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5823 - accuracy: 0.8190 - val_loss: 0.5825 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.87313\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5740 - accuracy: 0.8199 - val_loss: 0.5821 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.87313\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5735 - accuracy: 0.8205 - val_loss: 0.6038 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.87313\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5635 - accuracy: 0.8230 - val_loss: 0.5710 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.87313\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5570 - accuracy: 0.8208 - val_loss: 0.6046 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.87313\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5494 - accuracy: 0.8233 - val_loss: 0.5960 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.87313\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5397 - accuracy: 0.8264 - val_loss: 0.5771 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87313\n",
      "Epoch 21/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5379 - accuracy: 0.8243 - val_loss: 0.6121 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.87313\n",
      "Epoch 22/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5268 - accuracy: 0.8249 - val_loss: 0.5816 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.87313\n",
      "Epoch 23/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5123 - accuracy: 0.8255 - val_loss: 0.5766 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.87313\n",
      "Epoch 24/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5132 - accuracy: 0.8292 - val_loss: 0.5681 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.87313\n",
      "Epoch 25/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.5068 - accuracy: 0.8330 - val_loss: 0.5945 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.87313\n",
      "Epoch 26/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4962 - accuracy: 0.8292 - val_loss: 0.6069 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.87313\n",
      "Epoch 27/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4953 - accuracy: 0.8355 - val_loss: 0.6039 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.87313\n",
      "Epoch 28/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4880 - accuracy: 0.8370 - val_loss: 0.5825 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.87313\n",
      "Epoch 29/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4829 - accuracy: 0.8386 - val_loss: 0.6218 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.87313\n",
      "Epoch 30/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4681 - accuracy: 0.8364 - val_loss: 0.6186 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.87313\n",
      "Epoch 31/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4624 - accuracy: 0.8426 - val_loss: 0.5924 - val_accuracy: 0.8507\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.87313\n",
      "Epoch 32/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4520 - accuracy: 0.8445 - val_loss: 0.5828 - val_accuracy: 0.8507\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.87313\n",
      "Epoch 33/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4468 - accuracy: 0.8404 - val_loss: 0.5887 - val_accuracy: 0.8507\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.87313\n",
      "Epoch 34/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4434 - accuracy: 0.8479 - val_loss: 0.5619 - val_accuracy: 0.8507\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.87313\n",
      "Epoch 35/50\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.4327 - accuracy: 0.8432 - val_loss: 0.5971 - val_accuracy: 0.8507\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.87313\n",
      "Epoch 00035: early stopping\n"
     ]
    }
   ],
   "source": [
    "for i in range(18):\n",
    "    x, y = [],[[],[],[],[],[],[],[]]\n",
    "    for name in train_json[4019*i:4019*i+4019]:\n",
    "        with open(name, 'r',encoding='utf-8') as j:\n",
    "            data = json.load(j)\n",
    "            f_name = data['image_file_name']\n",
    "            img = cv2.imread(train_dir+ '/' + f_name, cv2.IMREAD_COLOR)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            for idx, n in enumerate(status):\n",
    "                i = idx + 1\n",
    "    \n",
    "                if data[n] == '0':\n",
    "                    y[i].append(0)\n",
    "                else:\n",
    "                    y[i].append(int(data[n]))\n",
    "            x.append(img)\n",
    "            \n",
    "\n",
    "    dump, y_1, y_2, y_3,y_4,y_5,y_6 = y\n",
    "    x = np.array(x)\n",
    "    x = x/255\n",
    "\n",
    "    y_1 = np.array(y_1).reshape(-1,1)\n",
    "    y_2 = np.array(y_2).reshape(-1,1)\n",
    "    y_3 = np.array(y_3).reshape(-1,1)\n",
    "    y_4 = np.array(y_4).reshape(-1,1)\n",
    "    y_5 = np.array(y_5).reshape(-1,1)\n",
    "    y_6 = np.array(y_6).reshape(-1,1)\n",
    "\n",
    "    x_train, x_test = train_test_split(x, test_size=0.2,random_state=123)\n",
    "    y_train_1, y_test_1 = train_test_split(y_1, test_size=0.2,random_state=123)\n",
    "\n",
    "    history = model1.fit(x_train, y_train_1,\n",
    "            epochs = 50,\n",
    "            callbacks=[es, mc],\n",
    "            batch_size = 16,\n",
    "            validation_data = (x_test, y_test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "epochs = range(1, len(loss)+1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Loss')\n",
    "plt.plot(epochs, history.history['loss'], 'b', label='train_loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'g', label='val_loss')\n",
    "plt.ylim([0,1.0])\n",
    "plt.grid(True)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Accuray')\n",
    "plt.plot(epochs, history.history['accuracy'], 'b', label='train_accuracy')\n",
    "plt.plot(epochs, history.history['val_accuracy'], 'g', label='val_accuracy')\n",
    "plt.ylim([0.7,1])\n",
    "plt.grid(True)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.load_model('cp/test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train = [],[[],[],[],[],[],[],[]]\n",
    "# for name in train_json[10000:20000]:\n",
    "#     with open(name, 'r',encoding='utf-8') as j:\n",
    "#         data = json.load(j)\n",
    "#         f_name = data['image_file_name']\n",
    "#         img = cv2.imread(train_dir+ '/' + f_name, cv2.IMREAD_COLOR)\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         for idx, n in enumerate(status):\n",
    "#             i = idx + 1\n",
    " \n",
    "#             if data[n] == '0':\n",
    "#                 y_train[i].append(0)\n",
    "#             else:\n",
    "#                 y_train[i].append(int(data[n]))\n",
    "#         x_train.append(img)\n",
    "        \n",
    "\n",
    "# dump, y_train_1, y_train_2, y_train_3,y_train_4,y_train_5,y_train_6 = y_train\n",
    "# x_train = np.array(x_train)\n",
    "# x_train = x_train/255\n",
    "\n",
    "# y_train_1 = np.array(y_train_1).reshape(-1,1)\n",
    "# y_train_2 = np.array(y_train_2).reshape(-1,1)\n",
    "# y_train_3 = np.array(y_train_3).reshape(-1,1)\n",
    "# y_train_4 = np.array(y_train_4).reshape(-1,1)\n",
    "# y_train_5 = np.array(y_train_5).reshape(-1,1)\n",
    "# y_train_6 = np.array(y_train_6).reshape(-1,1)\n",
    "\n",
    "# x_train, x_test = train_test_split(x_train, test_size=0.2)\n",
    "# y_train_1, y_test_1 = train_test_split(y_train_1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chaddol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a687caacb10a844a55fefd3b5f8dd537474e79b4cd9a59a4e226ce1d72456a88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
